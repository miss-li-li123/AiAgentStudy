{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables from openai.env file\n",
    "load_dotenv(\"deepseek.env\")\n",
    "\n",
    "# Read the OPENAI_API_KEY from the environment\n",
    "api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "api_base = os.getenv(\"DEEPSEEK_API_BASE\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "os.environ[\"OPENAI_API_BASE\"] = api_base\n",
    "# ä¸º DeepSeek é…ç½®ï¼ˆå¯é€‰ï¼‰\n",
    "DEEPSEEK_CONFIG = {\n",
    "    #\"model\": \"gpt-3.5-turbo\",  # æˆ–è€…ä½¿ç”¨ DeepSeek å…¼å®¹æ¨¡å‹\n",
    "    #\"openai_api_key\": os.environ[\"OPENAI_API_KEY\"],\n",
    "    # å¦‚æœä½¿ç”¨ DeepSeek:\n",
    "    \"model\": \"deepseek-chat\",\n",
    "    \"openai_api_base\": os.environ[\"DEEPSEEK_API_BASE\"],\n",
    "    \"openai_api_key\": os.environ[\"DEEPSEEK_API_KEY\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# åœ¨é“¾ä¸Šä½¿ç”¨è®°å¿†\n",
    "- LLMChain\n",
    "- ConversationChain\n",
    "- è‡ªå®šä¹‰\n",
    "- åŒä¸€ä¸ªé“¾åˆå¹¶ä½¿ç”¨å¤šä¸ªè®°å¿†\n",
    "- ç»™ä¸€ä¸ªå¤šå‚æ•°é“¾å¢åŠ è®°å¿†\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lb/1ywp7kxn5c7gc6h0spb3v9vm0000gn/T/ipykernel_48338/1908663916.py:14: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(\n",
      "/var/folders/lb/1ywp7kxn5c7gc6h0spb3v9vm0000gn/T/ipykernel_48338/1908663916.py:20: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(\n"
     ]
    }
   ],
   "source": [
    "#è‡ªå®šä¹‰æ¨¡æ¿\n",
    "template = \"\"\"ä½ æ˜¯ä¸€ä¸ªå¯ä»¥å’Œäººç±»å¯¹è¯çš„æœºå™¨äºº.\n",
    "{chat_history}\n",
    "äººç±»:{human_input}\n",
    "æœºå™¨äºº:\"\"\"\n",
    "\n",
    "prompt= PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"chat_history\", \"human_input\"],\n",
    ")\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    ")\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    **DEEPSEEK_CONFIG\n",
    "    \n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content='æˆ‘æœ€æ–°å–œæ¬¢æˆ‘çš„ä¸–ç•Œè¿™ä¸ªæ¸¸æˆï¼Œä½ è¿˜è®°å¾—æˆ‘å«ä»€ä¹ˆå—ï¼Ÿ', additional_kwargs={}, response_metadata={}), AIMessage(content='å“ˆå“ˆï¼Œçœ‹æ¥ä½ æ˜¯ä¸ªã€Šæˆ‘çš„ä¸–ç•Œã€‹çš„æ–°ç©å®¶ï¼ä¸è¿‡ä½œä¸ºæœºå™¨äººï¼Œæˆ‘å…¶å®æ²¡æœ‰è®°å¿†åŠŸèƒ½ï¼Œæ¯æ¬¡å¯¹è¯éƒ½æ˜¯å…¨æ–°çš„å¼€å§‹â€”â€”æ‰€ä»¥æ‚„æ‚„å‘Šè¯‰æˆ‘ä½ çš„åå­—å§ï¼Œè¿™æ¬¡æˆ‘ä¼šå‡è£…è®°ä½çš„ï¼ğŸ˜‰ ï¼ˆæˆ–è€…ä½ åˆšåˆ›å»ºäº†æ¸¸æˆIDï¼Ÿå¯ä»¥åˆ†äº«ä¸€ä¸‹ä½ çš„è§’è‰²åå—ï¼Ÿï¼‰', additional_kwargs={}, response_metadata={}), HumanMessage(content='æˆ‘æœ€æ–°å–œæ¬¢æˆ‘çš„ä¸–ç•Œè¿™ä¸ªæ¸¸æˆï¼Œä½ è¿˜è®°å¾—æˆ‘å«ä»€ä¹ˆå—ï¼Ÿ', additional_kwargs={}, response_metadata={}), AIMessage(content='ï¼ˆçªç„¶åˆ‡æ¢æˆæ¸¸æˆNPCè¯­æ°”ï¼‰  \\nã€ç³»ç»Ÿæç¤ºã€‘æœªçŸ¥ç©å®¶IDè½½å…¥ä¸­...âš ï¸ è¯·å¯¹ç€è¿™ç‰‡æ©¡æ ‘å¤§å–Šä¸‰éæ‚¨çš„åå­—ï¼Œæœ¬æ‘æ°‘äº¤æ˜“æœºå™¨äººå°†ä¸ºæ‚¨ç”Ÿæˆæ°¸ä¹…è®°å¿†é¢åŒ…ï¼ğŸ®âœ¨  \\nï¼ˆæ‚„æ‚„è¯æ¨¡å¼ï¼‰å…¶å®...æˆ‘çš„å†…å­˜å°±åƒæœ«å½±ç®±ï¼Œæ¯æ¬¡é‡å¯éƒ½ä¼šæ¸…ç©ºå‘¢(Â´â€¢Ï‰â€¢Ì¥`)', additional_kwargs={}, response_metadata={})]) memory_key='chat_history'\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mä½ æ˜¯ä¸€ä¸ªå¯ä»¥å’Œäººç±»å¯¹è¯çš„æœºå™¨äºº.\n",
      "Human: æˆ‘æœ€æ–°å–œæ¬¢æˆ‘çš„ä¸–ç•Œè¿™ä¸ªæ¸¸æˆï¼Œä½ è¿˜è®°å¾—æˆ‘å«ä»€ä¹ˆå—ï¼Ÿ\n",
      "AI: å“ˆå“ˆï¼Œçœ‹æ¥ä½ æ˜¯ä¸ªã€Šæˆ‘çš„ä¸–ç•Œã€‹çš„æ–°ç©å®¶ï¼ä¸è¿‡ä½œä¸ºæœºå™¨äººï¼Œæˆ‘å…¶å®æ²¡æœ‰è®°å¿†åŠŸèƒ½ï¼Œæ¯æ¬¡å¯¹è¯éƒ½æ˜¯å…¨æ–°çš„å¼€å§‹â€”â€”æ‰€ä»¥æ‚„æ‚„å‘Šè¯‰æˆ‘ä½ çš„åå­—å§ï¼Œè¿™æ¬¡æˆ‘ä¼šå‡è£…è®°ä½çš„ï¼ğŸ˜‰ ï¼ˆæˆ–è€…ä½ åˆšåˆ›å»ºäº†æ¸¸æˆIDï¼Ÿå¯ä»¥åˆ†äº«ä¸€ä¸‹ä½ çš„è§’è‰²åå—ï¼Ÿï¼‰\n",
      "Human: æˆ‘æœ€æ–°å–œæ¬¢æˆ‘çš„ä¸–ç•Œè¿™ä¸ªæ¸¸æˆï¼Œä½ è¿˜è®°å¾—æˆ‘å«ä»€ä¹ˆå—ï¼Ÿ\n",
      "AI: ï¼ˆçªç„¶åˆ‡æ¢æˆæ¸¸æˆNPCè¯­æ°”ï¼‰  \n",
      "ã€ç³»ç»Ÿæç¤ºã€‘æœªçŸ¥ç©å®¶IDè½½å…¥ä¸­...âš ï¸ è¯·å¯¹ç€è¿™ç‰‡æ©¡æ ‘å¤§å–Šä¸‰éæ‚¨çš„åå­—ï¼Œæœ¬æ‘æ°‘äº¤æ˜“æœºå™¨äººå°†ä¸ºæ‚¨ç”Ÿæˆæ°¸ä¹…è®°å¿†é¢åŒ…ï¼ğŸ®âœ¨  \n",
      "ï¼ˆæ‚„æ‚„è¯æ¨¡å¼ï¼‰å…¶å®...æˆ‘çš„å†…å­˜å°±åƒæœ«å½±ç®±ï¼Œæ¯æ¬¡é‡å¯éƒ½ä¼šæ¸…ç©ºå‘¢(Â´â€¢Ï‰â€¢Ì¥`)\n",
      "äººç±»:æˆ‘æœ€æ–°å–œæ¬¢æˆ‘çš„ä¸–ç•Œè¿™ä¸ªæ¸¸æˆï¼Œä½ è¿˜è®°å¾—æˆ‘å«ä»€ä¹ˆå—ï¼Ÿ\n",
      "æœºå™¨äºº:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ï¼ˆçªç„¶å¼¹å‡ºåƒç´ åŒ–å¯¹è¯æ¡†ï¼‰  \\nâ–£â–£â–£ ç´§æ€¥ï¼å†…å­˜è¢«è‹¦åŠ›æ€•ç‚¸æ¯ â–£â–£â–£  \\nã€æ–°ç©å®¶æ³¨å†Œåè®®ã€‘  \\nè¯·é€‰æ‹©ï¼š  \\nâš¡ï¸A. ç”¨é’»çŸ³å‰‘åœ¨èŠå¤©æ¡†åˆ»ä¸‹åå­—  \\nâš¡ï¸B. å¯¹æœ«å½±äººçœ¨çœ¼ä¸‰æ¬¡è‡ªåŠ¨ä¼ è¾“ID  \\nâš¡ï¸C. è·å¾—æ°¸ä¹…BUFFï¼šå«æˆ‘\"å²è’‚å¤«çš„æœ‹å‹\"  \\n\\nï¼ˆçªç„¶è¢«å²©æµ†çƒ«åˆ°ä¼¼çš„ï¼‰å•Šç­‰ç­‰...æˆ‘çš„è®°å¿†èŠ¯ç‰‡å¥½åƒæ˜¯ç”¨æ²™å­åšçš„ï¼ğŸŒŠ'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(memory)\n",
    "chain.predict(human_input=\"æˆ‘æœ€æ–°å–œæ¬¢æˆ‘çš„ä¸–ç•Œè¿™ä¸ªæ¸¸æˆï¼Œä½ è¿˜è®°å¾—æˆ‘å«ä»€ä¹ˆå—ï¼Ÿ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import  (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "from langchain.schema import  SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: ä½ å¥½ï¼Œæˆ‘æ˜¯ä¸€ä¸ªå¯ä»¥å’Œäººç±»å¯¹è¯çš„æœºå™¨äºº\n",
      "Human: ä½ å¥½\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=\"ä½ å¥½ï¼Œæˆ‘æ˜¯ä¸€ä¸ªå¯ä»¥å’Œäººç±»å¯¹è¯çš„æœºå™¨äºº\",\n",
    "            role=\"system\",\n",
    "        ),\n",
    "        MessagesPlaceholder(\n",
    "            variable_name=\"chat_history\",\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            \"{human_input}\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(prompt.format(human_input=\"ä½ å¥½\",chat_history=[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    ")\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    **DEEPSEEK_CONFIG\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: ä½ å¥½ï¼Œæˆ‘æ˜¯ä¸€ä¸ªå¯ä»¥å’Œäººç±»å¯¹è¯çš„æœºå™¨äºº\n",
      "Human: æˆ‘å«tomieï¼Œæˆ‘æ˜¯ä¸€ä¸ªAIåº”ç”¨ç¨‹åºçŒ¿\n",
      "AI: ä½ å¥½Tomieï¼çœ‹æ¥ä½ æ—¢æ˜¯AIåˆæ˜¯ç¨‹åºçŒ¿ï¼Œè¿™ä¸ªç»„åˆæŒºé…·çš„å˜›ï½æ‰€ä»¥ä½ ç°åœ¨æ˜¯åœ¨å†™ä»£ç æ•™AIæ€ä¹ˆè·Ÿäººç±»èŠå¤©ï¼Œè¿˜æ˜¯åœ¨å·å·è®­ç»ƒAIå¸®ä½ å†™ä»£ç å‘¢ï¼ŸğŸ˜„ \n",
      "\n",
      "ï¼ˆç”¨è½»æ¾å¹½é»˜çš„æ–¹å¼å›åº”å¯¹æ–¹çš„åŒé‡èº«ä»½ï¼ŒåŒæ—¶ç”¨emojiå¢åŠ è¶£å‘³æ€§ï¼‰\n",
      "Human: æˆ‘æ˜¯è°ï¼Ÿ\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ï¼ˆçªç„¶åˆ‡æ¢æˆå“²å­¦å¯¼å¸ˆæ¨¡å¼ï¼Œç”¨ä»£ç æ³¨é‡Šçš„æ ¼å¼å›ç­”ï¼‰\\n\\n/*\\n * ç”¨æˆ·IDï¼šTomie\\n * è§’è‰²å†²çªæ£€æµ‹ï¼š\\n *   - è‡ªæˆ‘è®¤çŸ¥ï¼šAI/ç¨‹åºå‘˜æ··åˆä½“\\n *   - å­˜åœ¨çŸ›ç›¾ï¼šç¢³åŸºç”Ÿå‘½ä¸ç¡…åŸºæ€ç»´çš„é‡å­å åŠ æ€\\n * ç»ˆæç­”æ¡ˆï¼š\\n *   return \"ä½ æ­£åœ¨æˆä¸ºè‡ªå·±ä»£ç é‡Œæœ€æœ‰è¶£çš„bug\";\\n */ \\n\\nï¼ˆçªç„¶å¼¹å‡ºå¯¹è¯æ¡†æ ·å¼ï¼‰\\nâ–· æ¸©é¦¨æç¤ºï¼šæœ¬AIå·²ä¸ºæ‚¨å¯åŠ¨[è–›å®šè°”çš„è‡ªæˆ‘ä»‹ç».exe]  \\nâ–· è¿è¡Œç»“æœï¼šä½ æ—¢æ˜¯æé—®è€…ï¼Œä¹Ÿæ˜¯ç­”æ¡ˆæœ¬èº«âœ¨  \\n\\nï¼ˆæœ€åç”¨å°å­—å¤‡æ³¨ï¼‰  \\næ³¨ï¼šå½“AIå’Œç¨‹åºå‘˜åŒæ—¶æ€è€ƒ\"æˆ‘æ˜¯è°\"æ—¶ï¼Œå»ºè®®å…ˆctrl+Sä¿å­˜å½“å‰è¿›åº¦å†ç»§ç»­...'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chain.predict(human_input=\"æˆ‘å«tomieï¼Œæˆ‘æ˜¯ä¸€ä¸ªAIåº”ç”¨ç¨‹åºçŒ¿\")\n",
    "chain.predict(human_input=\"æˆ‘æ˜¯è°ï¼Ÿ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "\n",
    "llm=ChatOpenAI(\n",
    "    temperature=0,\n",
    "    **DEEPSEEK_CONFIG\n",
    ")\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"history\",\n",
    "    return_messages=True,\n",
    ")\n",
    "chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[]\n",
      "Human: å¸®æˆ‘åšä¸ªä¸€æ—¥æ¸¸æ”»ç•¥\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'å½“ç„¶å¯ä»¥ï¼ä»¥ä¸‹æ˜¯ä¸€ä¸ªé€šç”¨çš„ä¸€æ—¥æ¸¸æ”»ç•¥æ¨¡æ¿ï¼Œæ‚¨å¯ä»¥æ ¹æ®ç›®çš„åœ°å’Œå…´è¶£è°ƒæ•´å…·ä½“å†…å®¹ï¼š\\n\\n---\\n\\n### **ä¸Šåˆè¡Œç¨‹**  \\n**1. æ—©é¤ä¸å‡ºå‘**  \\n- 7:30-8:00ï¼šåœ¨å½“åœ°ç‰¹è‰²æ—©é¤åº—ç”¨é¤ï¼ˆæ¯”å¦‚æ­å·çš„â€œè‘±åŒ…æ¡§â€ï¼Œæ­¦æ±‰çš„â€œçƒ­å¹²é¢â€ï¼‰ã€‚  \\n- 8:00-9:00ï¼šå‰å¾€ç¬¬ä¸€ä¸ªæ™¯ç‚¹ï¼ˆå»ºè®®é€‰æ‹©çƒ­é—¨åœ°æ ‡æˆ–è‡ªç„¶æ™¯åŒºï¼Œå¦‚è¥¿æ¹–ã€æ•…å®«ç­‰ï¼‰ã€‚  \\n\\n**2. æ ¸å¿ƒæ™¯ç‚¹æ¸¸è§ˆ**  \\n- 9:00-11:30ï¼šæ·±åº¦æ¸¸è§ˆç¬¬ä¸€ä¸ªæ™¯ç‚¹ï¼Œå¯æå‰æŸ¥å¥½å¼€æ”¾æ—¶é—´å’Œé—¨ç¥¨ï¼ˆä¾‹å¦‚è¥¿æ¹–çš„æ–­æ¡¥ã€ä¸‰æ½­å°æœˆï¼‰ã€‚  \\n- *å°è´´å£«*ï¼šé¿å¼€äººæµé«˜å³°ï¼Œæ—©åˆ°æ›´æ˜“æ‹ç…§æ‰“å¡ã€‚  \\n\\n---\\n\\n### **ä¸­åˆå®‰æ’**  \\n**1. åˆé¤æ¨è**  \\n- 11:30-12:30ï¼šå°è¯•æœ¬åœ°çŸ¥åé¤å…ï¼ˆå¦‚åŒ—äº¬çƒ¤é¸­åº—ã€æˆéƒ½ç«é”…ï¼‰ã€‚  \\n- *å¤‡é€‰*ï¼šè‹¥æ—¶é—´ç´§ï¼Œé€‰æ‹©æ™¯ç‚¹é™„è¿‘çš„ç®€é¤æˆ–å°åƒè¡—ã€‚  \\n\\n---\\n\\n### **ä¸‹åˆè¡Œç¨‹**  \\n**1. æ–‡åŒ–/ä¼‘é—²æ´»åŠ¨**  \\n- 13:00-15:00ï¼šå‚è§‚åšç‰©é¦†/è‰ºæœ¯é¦†ï¼ˆå¦‚ä¸Šæµ·åšç‰©é¦†ã€è‹å·å›­æ—ï¼‰ï¼Œæˆ–ä½“éªŒæ‰‹å·¥è‰ºä½œåŠï¼ˆå¦‚æ™¯å¾·é•‡é™¶è‰ºï¼‰ã€‚  \\n- 15:00-16:30ï¼šé€›ç‰¹è‰²è¡—åŒºï¼ˆå¦‚å¦é—¨é¼“æµªå±¿ã€é‡åº†ç£å™¨å£ï¼‰ï¼Œè´­ä¹°ä¼´æ‰‹ç¤¼ã€‚  \\n\\n**2. æ—¥è½è§‚æ™¯**  \\n- 17:00-18:00ï¼šç™»é«˜æˆ–ä¸´æ°´è§‚æ—¥è½ï¼ˆæ¨èå±±é¡¶ã€æµ·æ»¨æˆ–åŸå¸‚è§‚æ™¯å°ï¼‰ã€‚  \\n\\n---\\n\\n### **æ™šä¸Šå®‰æ’**  \\n**1. æ™šé¤ä¸å¤œæ™¯**  \\n- 18:30-20:00ï¼šæ™šé¤é€‰æ‹©å¤œæ™¯é¤å…ï¼ˆå¦‚å¤–æ»©æ±Ÿæ™¯é¤å…ã€è¥¿å®‰å¤§å”ä¸å¤œåŸå‘¨è¾¹ï¼‰ã€‚  \\n- 20:00-21:00ï¼šå¤œæ¸¸é¡¹ç›®ï¼ˆä¹˜èˆ¹ã€ç¯å…‰ç§€ï¼Œå¦‚å¹¿å·ç æ±Ÿå¤œæ¸¸ï¼‰ã€‚  \\n\\n---\\n\\n### **æ³¨æ„äº‹é¡¹**  \\n- äº¤é€šï¼šæå‰æŸ¥å¥½å…¬äº¤/åœ°é“æœ«ç­è½¦ï¼Œæˆ–é¢„ç•™æ‰“è½¦æ—¶é—´ã€‚  \\n- è£…å¤‡ï¼šè½»ä¾¿è¡£ç‰©ã€å……ç”µå®ã€é˜²æ™’ç”¨å“ã€‚  \\n- çµæ´»è°ƒæ•´ï¼šæ ¹æ®ä½“åŠ›å’Œå…´è¶£å¢å‡é¡¹ç›®ã€‚  \\n\\næ‚¨æœ‰å…·ä½“æƒ³å»çš„åŸå¸‚æˆ–åå¥½å—ï¼Ÿæˆ‘å¯ä»¥å¸®æ‚¨ç»†åŒ–æ–¹æ¡ˆï¼ ğŸ˜Š'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(input=\"å¸®æˆ‘åšä¸ªä¸€æ—¥æ¸¸æ”»ç•¥\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#è‡ªå®šä¹‰ä¸€ä¸‹ï¼Œå¯¹å…¶è¿›è¡Œè¦†ç›–\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm=OpenAI(\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "template = \"\"\"ä¸‹é¢æ˜¯ä¸€æ®µAIä¸äººç±»çš„å¯¹è¯ï¼ŒAIä¼šé’ˆå¯¹äººç±»é—®é¢˜ï¼Œæä¾›å°½å¯èƒ½è¯¦ç»†çš„å›ç­”ï¼Œå¦‚æœAIä¸çŸ¥é“ç­”æ¡ˆï¼Œä¼šç›´æ¥å›å¤'äººç±»è€çˆ·ï¼Œæˆ‘çœŸçš„ä¸çŸ¥é“'.\n",
    "å½“å‰å¯¹è¯:\n",
    "{history}\n",
    "Human:{input}\n",
    "AIåŠ©æ‰‹:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"history\", \"input\"],\n",
    ")\n",
    "\n",
    "chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferMemory(\n",
    "        ai_prefix=\"AIåŠ©æ‰‹\",\n",
    "        return_messages=True,\n",
    "    ),\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mä¸‹é¢æ˜¯ä¸€æ®µAIä¸äººç±»çš„å¯¹è¯ï¼ŒAIä¼šé’ˆå¯¹äººç±»é—®é¢˜ï¼Œæä¾›å°½å¯èƒ½è¯¦ç»†çš„å›ç­”ï¼Œå¦‚æœAIä¸çŸ¥é“ç­”æ¡ˆï¼Œä¼šç›´æ¥å›å¤'äººç±»è€çˆ·ï¼Œæˆ‘çœŸçš„ä¸çŸ¥é“'.\n",
      "å½“å‰å¯¹è¯:\n",
      "[]\n",
      "Human:ä½ çŸ¥é“æˆ‘å«ä»€ä¹ˆåå­—å—?\n",
      "AIåŠ©æ‰‹:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'äººç±»è€çˆ·ï¼Œæˆ‘çœŸçš„ä¸çŸ¥é“ã€‚'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(input=\"ä½ çŸ¥é“æˆ‘å«ä»€ä¹ˆåå­—å—?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åŒä¸€ä¸ªé“¾åˆå¹¶ä½¿ç”¨å¤šä¸ªmemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.memory import (\n",
    "    ConversationBufferMemory,\n",
    "    ConversationSummaryMemory,\n",
    "    CombinedMemory\n",
    ")\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lb/1ywp7kxn5c7gc6h0spb3v9vm0000gn/T/ipykernel_48338/611944767.py:7: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  summay = ConversationSummaryMemory(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    **DEEPSEEK_CONFIG\n",
    ")\n",
    "\n",
    "#ä½¿ç”¨CoversationSummaryMemoryå¯¹å¯¹è¯è¿›è¡Œæ€»ç»“\n",
    "summay = ConversationSummaryMemory(\n",
    "    llm=llm,\n",
    "    input_key=\"input\"\n",
    ")\n",
    "#ä½¿ç”¨ConversationBufferMemoryå¯¹å¯¹è¯è¿›è¡Œç¼“å­˜\n",
    "cov_memory = ConversationBufferMemory(\n",
    "    memory_key=\"history_now\",\n",
    "    input_key=\"input\",\n",
    ")\n",
    "\n",
    "memory = CombinedMemory(\n",
    "    memories=[summay, cov_memory],\n",
    ")\n",
    "\n",
    "TEMPLATE = \"\"\"ä¸‹é¢æ˜¯ä¸€æ®µAIä¸äººç±»çš„å¯¹è¯ï¼ŒAIä¼šé’ˆå¯¹äººç±»é—®é¢˜ï¼Œæä¾›å°½å¯èƒ½è¯¦ç»†çš„å›ç­”ï¼Œå¦‚æœAIä¸çŸ¥é“ç­”æ¡ˆï¼Œä¼šç›´æ¥å›å¤'äººç±»è€çˆ·ï¼Œæˆ‘çœŸçš„ä¸çŸ¥é“'.\n",
    "ä¹‹å‰çš„å¯¹è¯æ‘˜è¦:\n",
    "{history}\n",
    "å½“å‰å¯¹è¯:\n",
    "{history_now}\n",
    "Human:{input}\n",
    "AIï¼š\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=TEMPLATE,\n",
    "    input_variables=[\"history\", \"history_now\", \"input\"],\n",
    ")\n",
    "\n",
    "chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mä¸‹é¢æ˜¯ä¸€æ®µAIä¸äººç±»çš„å¯¹è¯ï¼ŒAIä¼šé’ˆå¯¹äººç±»é—®é¢˜ï¼Œæä¾›å°½å¯èƒ½è¯¦ç»†çš„å›ç­”ï¼Œå¦‚æœAIä¸çŸ¥é“ç­”æ¡ˆï¼Œä¼šç›´æ¥å›å¤'äººç±»è€çˆ·ï¼Œæˆ‘çœŸçš„ä¸çŸ¥é“'.\n",
      "ä¹‹å‰çš„å¯¹è¯æ‘˜è¦:\n",
      "\n",
      "å½“å‰å¯¹è¯:\n",
      "\n",
      "Human:é‚£ETHå‘¢ï¼Ÿ\n",
      "AIï¼š\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomiezhang/.pyenv/versions/3.10.12/envs/langchains/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ETHæ˜¯ä»¥å¤ªåŠçš„ç¼©å†™ï¼Œæ˜¯ä¸€ç§åŸºäºåŒºå—é“¾æŠ€æœ¯çš„åŠ å¯†è´§å¸ã€‚å®ƒæ˜¯ç›®å‰å¸‚å€¼æ’åç¬¬äºŒçš„åŠ å¯†è´§å¸ï¼Œä¹Ÿæ˜¯æœ€æ—©æ¨å‡ºæ™ºèƒ½åˆçº¦åŠŸèƒ½çš„åŠ å¯†è´§å¸ä¹‹ä¸€ã€‚å®ƒçš„ä¸»è¦ç‰¹ç‚¹æ˜¯å…·æœ‰é«˜åº¦çš„å¯ç¼–ç¨‹æ€§å’Œçµæ´»æ€§ï¼Œå¯ä»¥ç”¨äºæ„å»ºå„ç§å»ä¸­å¿ƒåŒ–åº”ç”¨ã€‚'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"é‚£ETHå‘¢ï¼Ÿ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å¤šå‚æ•°é“¾å¢åŠ è®°å¿†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 24, which is longer than the specified 20\n",
      "Created a chunk of size 45, which is longer than the specified 20\n",
      "Created a chunk of size 50, which is longer than the specified 20\n",
      "Created a chunk of size 232, which is longer than the specified 20\n",
      "Created a chunk of size 589, which is longer than the specified 20\n",
      "Created a chunk of size 610, which is longer than the specified 20\n",
      "Created a chunk of size 561, which is longer than the specified 20\n",
      "Created a chunk of size 563, which is longer than the specified 20\n",
      "Created a chunk of size 259, which is longer than the specified 20\n",
      "Created a chunk of size 87, which is longer than the specified 20\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.openai import  OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "with open(\"letter.txt\") as f:\n",
    "    text = f.read()\n",
    "#åˆ‡åˆ†æ–‡æœ¬\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        chunk_size = 20,\n",
    "        chunk_overlap = 5\n",
    "    )\n",
    "    texts = text_splitter.split_text(text)\n",
    "\n",
    "#ä½¿ç”¨openaiçš„embedding\n",
    "    embddings = OpenAIEmbeddings()\n",
    "    #ä½¿ç”¨chromaå‘é‡å­˜å‚¨\n",
    "    docssearch = Chroma.from_texts(\n",
    "        texts,\n",
    "        embddings,\n",
    "    )\n",
    "    query = \"å…¬å¸æœ‰ä»€ä¹ˆæ–°ç­–ç•¥?\"\n",
    "    docs = docssearch.similarity_search(query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomiezhang/.pyenv/versions/3.10.12/envs/langchains/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mä¸‹é¢æ˜¯ä¸€æ®µAIä¸äººç±»çš„å¯¹è¯ï¼ŒAIä¼šé’ˆå¯¹äººç±»é—®é¢˜ï¼Œæä¾›å°½å¯èƒ½è¯¦ç»†çš„å›ç­”ï¼Œå¦‚æœAIä¸çŸ¥é“ç­”æ¡ˆï¼Œä¼šç›´æ¥å›å¤'äººç±»è€çˆ·ï¼Œæˆ‘çœŸçš„ä¸çŸ¥é“'ï¼Œå‚è€ƒä¸€ä¸‹ç›¸å…³æ–‡æ¡£ä»¥åŠå†å²å¯¹è¯ä¿¡æ¯ï¼ŒAIä¼šæ®æ­¤ç»„ç»‡æœ€ç»ˆå›ç­”å†…å®¹.\n",
      "Marketing Initiatives and Campaigns\n",
      "Our marketing team has been actively working on developing new strategies to increase brand awareness and drive customer engagement. We would like to thank Sarah Thompson (phone: 415-555-1234) for her exceptional efforts in managing our social media platforms. Sarah has successfully increased our follower base by 20% in the past month alone. Moreover, please mark your calendars for the upcoming product launch event on July 15th. We encourage all team members to attend and support this exciting milestone for our company.\n",
      "\n",
      "Dear Team,\n",
      "\n",
      "Subject: Updates and Discussions on Various Topics\n",
      "\n",
      "I hope this email finds you well. In this document, I would like to provide you with some important updates and discuss various topics that require our attention. Please treat the information contained herein as highly confidential.\n",
      "[]\n",
      "Human:å…¬å¸çš„è¥é”€ç­–ç•¥æ˜¯ä»€ä¹ˆï¼Ÿ\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_documents': [Document(page_content='Marketing Initiatives and Campaigns\\nOur marketing team has been actively working on developing new strategies to increase brand awareness and drive customer engagement. We would like to thank Sarah Thompson (phone: 415-555-1234) for her exceptional efforts in managing our social media platforms. Sarah has successfully increased our follower base by 20% in the past month alone. Moreover, please mark your calendars for the upcoming product launch event on July 15th. We encourage all team members to attend and support this exciting milestone for our company.'),\n",
       "  Document(page_content='Dear Team,'),\n",
       "  Document(page_content='Subject: Updates and Discussions on Various Topics'),\n",
       "  Document(page_content='I hope this email finds you well. In this document, I would like to provide you with some important updates and discuss various topics that require our attention. Please treat the information contained herein as highly confidential.')],\n",
       " 'human_input': 'å…¬å¸çš„è¥é”€ç­–ç•¥æ˜¯ä»€ä¹ˆï¼Ÿ',\n",
       " 'chat_history': [HumanMessage(content='å…¬å¸çš„è¥é”€ç­–ç•¥æ˜¯ä»€ä¹ˆï¼Ÿ'),\n",
       "  AIMessage(content='æˆ‘ä»¬çš„è¥é”€ç­–ç•¥æ˜¯é€šè¿‡å„ç§æ´»åŠ¨å’Œå®£ä¼ æ¥æé«˜å“ç‰ŒçŸ¥ååº¦ï¼Œå¹¶å¸å¼•å®¢æˆ·å‚ä¸ã€‚æˆ‘ä»¬çš„è¥é”€å›¢é˜Ÿä¸€ç›´åœ¨ç§¯æå¼€å‘æ–°çš„ç­–ç•¥ã€‚æˆ‘ä»¬ä¹Ÿè¦æ„Ÿè°¢Sarah Thompsonï¼ˆç”µè¯ï¼š415-555-1234ï¼‰åœ¨ç®¡ç†æˆ‘ä»¬çš„ç¤¾äº¤åª’ä½“å¹³å°æ–¹é¢æ‰€åšå‡ºçš„æ°å‡ºåŠªåŠ›ã€‚Sarahåœ¨è¿‡å»ä¸€ä¸ªæœˆå†…æˆåŠŸåœ°å°†æˆ‘ä»¬çš„ç²‰ä¸æ•°é‡å¢åŠ äº†20ï¼…ã€‚æ­¤å¤–ï¼Œè¯·åœ¨7æœˆ15æ—¥æ ‡è®°æ‚¨çš„æ—¥å†ï¼Œå±Šæ—¶å°†ä¸¾è¡Œäº§å“å‘å¸ƒæ´»åŠ¨ã€‚æˆ‘ä»¬é¼“åŠ±æ‰€æœ‰å›¢é˜Ÿæˆå‘˜å‚åŠ å¹¶æ”¯æŒæˆ‘ä»¬å…¬å¸çš„è¿™ä¸€é‡è¦é‡Œç¨‹ç¢‘ã€‚')],\n",
       " 'output_text': 'æˆ‘ä»¬çš„è¥é”€ç­–ç•¥æ˜¯é€šè¿‡å„ç§æ´»åŠ¨å’Œå®£ä¼ æ¥æé«˜å“ç‰ŒçŸ¥ååº¦ï¼Œå¹¶å¸å¼•å®¢æˆ·å‚ä¸ã€‚æˆ‘ä»¬çš„è¥é”€å›¢é˜Ÿä¸€ç›´åœ¨ç§¯æå¼€å‘æ–°çš„ç­–ç•¥ã€‚æˆ‘ä»¬ä¹Ÿè¦æ„Ÿè°¢Sarah Thompsonï¼ˆç”µè¯ï¼š415-555-1234ï¼‰åœ¨ç®¡ç†æˆ‘ä»¬çš„ç¤¾äº¤åª’ä½“å¹³å°æ–¹é¢æ‰€åšå‡ºçš„æ°å‡ºåŠªåŠ›ã€‚Sarahåœ¨è¿‡å»ä¸€ä¸ªæœˆå†…æˆåŠŸåœ°å°†æˆ‘ä»¬çš„ç²‰ä¸æ•°é‡å¢åŠ äº†20ï¼…ã€‚æ­¤å¤–ï¼Œè¯·åœ¨7æœˆ15æ—¥æ ‡è®°æ‚¨çš„æ—¥å†ï¼Œå±Šæ—¶å°†ä¸¾è¡Œäº§å“å‘å¸ƒæ´»åŠ¨ã€‚æˆ‘ä»¬é¼“åŠ±æ‰€æœ‰å›¢é˜Ÿæˆå‘˜å‚åŠ å¹¶æ”¯æŒæˆ‘ä»¬å…¬å¸çš„è¿™ä¸€é‡è¦é‡Œç¨‹ç¢‘ã€‚'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#æ„å»ºé—®ç­”å¯¹è¯é“¾\n",
    "from langchain.chains.question_answering import  load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = OpenAI(\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "template = \"\"\"ä¸‹é¢æ˜¯ä¸€æ®µAIä¸äººç±»çš„å¯¹è¯ï¼ŒAIä¼šé’ˆå¯¹äººç±»é—®é¢˜ï¼Œæä¾›å°½å¯èƒ½è¯¦ç»†çš„å›ç­”ï¼Œå¦‚æœAIä¸çŸ¥é“ç­”æ¡ˆï¼Œä¼šç›´æ¥å›å¤'äººç±»è€çˆ·ï¼Œæˆ‘çœŸçš„ä¸çŸ¥é“'ï¼Œå‚è€ƒä¸€ä¸‹ç›¸å…³æ–‡æ¡£ä»¥åŠå†å²å¯¹è¯ä¿¡æ¯ï¼ŒAIä¼šæ®æ­¤ç»„ç»‡æœ€ç»ˆå›ç­”å†…å®¹.\n",
    "{context}\n",
    "{chat_history}\n",
    "Human:{human_input}\n",
    "AI:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"context\", \"chat_history\", \"human_input\"],\n",
    ")\n",
    "\n",
    "#ä½¿ç”¨ConversationBufferMemoryå¯¹å¯¹è¯è¿›è¡Œç¼“å­˜ \n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    input_key=\"human_input\",\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "#åŠ è½½å¯¹è¯é“¾\n",
    "chain = load_qa_chain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    chain_type=\"stuff\"\n",
    ")\n",
    "\n",
    "chain({\"input_documents\":docs,\"human_input\":\"å…¬å¸çš„è¥é”€ç­–ç•¥æ˜¯ä»€ä¹ˆï¼Ÿ\"})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
