{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables from openai.env file\n",
    "load_dotenv(\"deepseek.env\")\n",
    "\n",
    "# Read the OPENAI_API_KEY from the environment\n",
    "api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "api_base = os.getenv(\"DEEPSEEK_API_BASE\")\n",
    "serp_api_key = os.getenv(\"SERPAPI_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "os.environ[\"OPENAI_API_BASE\"] = api_base\n",
    "os.environ[\"SERPAPI_API_KEY\"] =serp_api_key\n",
    "# ä¸º DeepSeek é…ç½®ï¼ˆå¯é€‰ï¼‰\n",
    "DEEPSEEK_CONFIG = {\n",
    "    #\"model\": \"gpt-3.5-turbo\",  # æˆ–è€…ä½¿ç”¨ DeepSeek å…¼å®¹æ¨¡å‹\n",
    "    #\"openai_api_key\": os.environ[\"OPENAI_API_KEY\"],\n",
    "    # å¦‚æœä½¿ç”¨ DeepSeek:\n",
    "    \"model\": \"deepseek-chat\",\n",
    "    \"openai_api_base\": os.environ[\"DEEPSEEK_API_BASE\"],\n",
    "    \"openai_api_key\": os.environ[\"DEEPSEEK_API_KEY\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç¬¬ä¸€ä¸ªAgent\n",
    "- æ³¨æ„å½“å‰ä½¿ç”¨çš„langchainç‰ˆæœ¬ä¸º0.1.4ï¼Œè°ƒç”¨æ–¹å¼å’Œä¹‹å‰æ—§ç‰ˆæœ¬æœ‰æ‰€å·®å¼‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lb/1ywp7kxn5c7gc6h0spb3v9vm0000gn/T/ipykernel_69139/1091633627.py:7: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(**DEEPSEEK_CONFIG, temperature=0)\n",
      "/opt/miniconda3/lib/python3.12/site-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mä½ å¥½ï¼ğŸ˜Š æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿ\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'ä½ å¥½', 'output': 'ä½ å¥½ï¼ğŸ˜Š æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿ'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain_community.chat_models  import ChatOpenAI\n",
    "from langchain import hub\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import create_openai_functions_agent #ä¸åŒçš„agentæœ‰ä¸åŒçš„åˆ›å»ºæ–¹å¼\n",
    "from langchain.agents import AgentExecutor\n",
    "#åˆ›å»ºLLM \n",
    "llm = ChatOpenAI(**DEEPSEEK_CONFIG, temperature=0)\n",
    "#å®šä¹‰agentçš„prompt\n",
    "#https://smith.langchain.com/hub/hwchase17/openai-functions-agent\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "#å®šä¹‰å·¥å…·,åŠ è½½é¢„åˆ¶çš„å·¥å…·,æ³¨æ„æœ‰çš„å·¥å…·éœ€è¦æä¾›LLM\n",
    "tools = load_tools([\"llm-math\"], llm=llm)\n",
    "#åˆ›å»ºagent\n",
    "agent = create_openai_functions_agent(llm, tools, prompt)\n",
    "#å®šä¹‰agentçš„æ‰§è¡Œå™¨ï¼Œè¿™é‡Œæ³¨æ„ä¸è€ç‰ˆæœ¬çš„ä¸åŒ\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "agent_executor.invoke({\"input\": \"ä½ å¥½\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent\n",
    "- ä¸­é—´æ­¥éª¤å¤„ç†\n",
    "- æç¤ºè¯\n",
    "- æ¨¡å‹é…ç½®(åœæ­¢ç¬¦å¿…è¦çš„è¯)\n",
    "- è¾“å‡ºè§£æå™¨\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents import tool  # æ·»åŠ è¿™è¡Œå¯¼å…¥\n",
    "from langchain.agents.output_parsers import XMLAgentOutputParser\n",
    "from langchain_community.chat_models  import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructuredTool(name='search', description='å½“éœ€è¦äº†è§£æœ€æ–°çš„å¤©æ°”ä¿¡æ¯çš„æ—¶å€™æ‰ä¼šä½¿ç”¨è¿™ä¸ªå·¥å…·ã€‚', args_schema=<class 'langchain_core.utils.pydantic.search'>, func=<function search at 0x11d4d5080>)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#é…ç½®æ¨¡å‹\n",
    "model = ChatOpenAI(\n",
    "    **DEEPSEEK_CONFIG,\n",
    "    temperature=0,\n",
    "    )\n",
    "#å¯ç”¨å·¥å…·\n",
    "@tool\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"å½“éœ€è¦äº†è§£æœ€æ–°çš„å¤©æ°”ä¿¡æ¯çš„æ—¶å€™æ‰ä¼šä½¿ç”¨è¿™ä¸ªå·¥å…·ã€‚\"\"\"\n",
    "    return \"æ™´æœ—,32æ‘„æ°åº¦,æ— é£\"\n",
    "tool_list = [search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.12/site-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['agent_scratchpad', 'input', 'tools'], input_types={}, partial_variables={'chat_history': ''}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'xml-agent-convo', 'lc_hub_commit_hash': '00f6b7470fa25a24eef6e4e3c1e44ba07189f3e91c4d987223ad232490673be8'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['agent_scratchpad', 'chat_history', 'input', 'tools'], input_types={}, partial_variables={}, template=\"You are a helpful assistant. Help the user answer any questions.\\n\\nYou have access to the following tools:\\n\\n{tools}\\n\\nIn order to use a tool, you can use <tool></tool> and <tool_input></tool_input> tags. You will then get back a response in the form <observation></observation>\\nFor example, if you have a tool called 'search' that could run a google search, in order to search for the weather in SF you would respond:\\n\\n<tool>search</tool><tool_input>weather in SF</tool_input>\\n<observation>64 degrees</observation>\\n\\nWhen you are done, respond with a final answer between <final_answer></final_answer>. For example:\\n\\n<final_answer>The weather in SF is 64 degrees</final_answer>\\n\\nBegin!\\n\\nPrevious Conversation:\\n{chat_history}\\n\\nQuestion: {input}\\n{agent_scratchpad}\"), additional_kwargs={})])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#æç¤ºè¯æ¨¡ç‰ˆ\n",
    "# https://smith.langchain.com/hub\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/xml-agent-convo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸­é—´æ­¥éª¤ï¼Œå®ç°ä¸€ä¸ªlog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_intermediate_steps(intermediate_steps):\n",
    "    log = \"\"\n",
    "    for action, observation in intermediate_steps:\n",
    "        log += (\n",
    "            f\"<tool>{action.tool}</tool><tool_input>{action.tool_input}\"\n",
    "            f\"</tool_input><observation>{observation}</observation>\"\n",
    "        )\n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#å°†å·¥å…·åˆ—è¡¨æ’å…¥åˆ°æ¨¡ç‰ˆä¸­\n",
    "def convert_tools(tools):\n",
    "    return \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in tools])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å®šä¹‰agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: convert_intermediate_steps(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "    }\n",
    "    | prompt.partial(tools=convert_tools(tool_list))\n",
    "    | model.bind(stop=[\"</tool_input>\", \"</final_answer>\"])\n",
    "    | XMLAgentOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  agent_scratchpad: RunnableLambda(lambda x: format_to_openai_function_messages(x['intermediate_steps']))\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')])\n",
       "| RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x14e8aef10>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x15a499fd0>, model_name='gpt-4', temperature=0.0, openai_api_key='sk-pQ7osw5XrDY1Ejsx99EcEe5f58D04dDeB387B8E6B3C10bDb', openai_api_base='https://ai-yyds.com/v1', openai_proxy='https://ai-yyds.com/v1'), kwargs={'functions': [{'name': 'Calculator', 'description': 'Useful for when you need to answer questions about math.', 'parameters': {'properties': {'__arg1': {'title': '__arg1', 'type': 'string'}}, 'required': ['__arg1'], 'type': 'object'}}]})\n",
       "| OpenAIFunctionsAgentOutputParser()"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#æ‰§è¡Œagent\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tool_list, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m<tool>search</tool><tool_input>åŒ—äº¬ä»Šå¤©çš„å¤©æ°”\u001b[0m\u001b[36;1m\u001b[1;3mæ™´æœ—,32æ‘„æ°åº¦,æ— é£\u001b[0m\u001b[32;1m\u001b[1;3m<final_answer>åŒ—äº¬ä»Šå¤©çš„å¤©æ°”æ˜¯æ™´æœ—ï¼Œæ¸©åº¦ä¸º32æ‘„æ°åº¦ï¼Œæ— é£ã€‚\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'åŒ—äº¬ä»Šå¤©çš„å¤©æ°”å¦‚ä½•?', 'output': 'åŒ—äº¬ä»Šå¤©çš„å¤©æ°”æ˜¯æ™´æœ—ï¼Œæ¸©åº¦ä¸º32æ‘„æ°åº¦ï¼Œæ— é£ã€‚'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"åŒ—äº¬ä»Šå¤©çš„å¤©æ°”å¦‚ä½•?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## agentTypeæ›´æ–°ç‰ˆ\n",
    "![agtype.png](agtype.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸€ä¸ªæ¯”è¾ƒå¤æ‚çš„agent\n",
    "- å·¥å…·\n",
    "- æ£€ç´¢å¢å¼ºRAG\n",
    "- è®°å¿†\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3508: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "# Import things that are needed generically\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain.tools import BaseTool, StructuredTool, tool\n",
    "from langchain_community.utilities import SerpAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search\n",
      "å½“éœ€è¦æŸ¥æ‰¾å®æ—¶ä¿¡æ¯çš„æ—¶å€™æ‰ä¼šä½¿ç”¨è¿™ä¸ªå·¥å…·.\n",
      "{'query': {'title': 'Query', 'type': 'string'}}\n"
     ]
    }
   ],
   "source": [
    "@tool\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"å½“éœ€è¦æŸ¥æ‰¾å®æ—¶ä¿¡æ¯çš„æ—¶å€™æ‰ä¼šä½¿ç”¨è¿™ä¸ªå·¥å…·.\"\"\"\n",
    "    serp = SerpAPIWrapper()\n",
    "    return serp.run(query)\n",
    "\n",
    "print(search.name)\n",
    "print(search.description)\n",
    "print(search.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'type': 'weather_result', 'temperature': '53', 'unit': 'Fahrenheit', 'precipitation': '0%', 'humidity': '53%', 'wind': '5 mph', 'location': 'ä¸­å›½åŒ—äº¬å¸‚', 'date': 'Saturday', 'weather': 'Partly cloudy'}\""
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(\"åŒ—äº¬ä»Šå¤©çš„å¤©æ°”å¦‚ä½•?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 documents\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'event_id': '30-clou-94-20250810163811-33a2a4c9', 'error_msg': 'Not Found. Please check the configuration.'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(docs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m documents\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m documents \u001b[38;5;241m=\u001b[39m RecursiveCharacterTextSplitter(\n\u001b[1;32m     11\u001b[0m     chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m\n\u001b[1;32m     12\u001b[0m )\u001b[38;5;241m.\u001b[39msplit_documents(docs)\n\u001b[0;32m---> 13\u001b[0m vector \u001b[38;5;241m=\u001b[39m \u001b[43mFAISS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOpenAIEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m retriever \u001b[38;5;241m=\u001b[39m vector\u001b[38;5;241m.\u001b[39mas_retriever()\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/langchain_core/vectorstores/base.py:848\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[0;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ids):\n\u001b[1;32m    846\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ids\n\u001b[0;32m--> 848\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/langchain_community/vectorstores/faiss.py:1043\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_texts\u001b[39m(\n\u001b[1;32m   1018\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1024\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FAISS:\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m \n\u001b[1;32m   1027\u001b[0m \u001b[38;5;124;03m    This is a user friendly interface that:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;124;03m            faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1043\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1044\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__from(\n\u001b[1;32m   1045\u001b[0m         texts,\n\u001b[1;32m   1046\u001b[0m         embeddings,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1051\u001b[0m     )\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/embeddings/base.py:591\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[0;34m(self, texts, chunk_size, **kwargs)\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[1;32m    590\u001b[0m engine \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeployment)\n\u001b[0;32m--> 591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/langchain_openai/embeddings/base.py:479\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[0;34m(self, texts, engine, chunk_size, **kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m batched_embeddings: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n\u001b[0;32m--> 479\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_chunk_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mclient_kwargs\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    483\u001b[0m         response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mmodel_dump()\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/openai/resources/embeddings.py:132\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[0;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m             embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[1;32m    127\u001b[0m                 base64\u001b[38;5;241m.\u001b[39mb64decode(data), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    128\u001b[0m             )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[0;32m--> 132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py:1259\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1246\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1247\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1254\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1255\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1256\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1257\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1258\u001b[0m     )\n\u001b[0;32m-> 1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/openai/_base_client.py:1047\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1044\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1046\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1047\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Error code: 404 - {'event_id': '30-clou-94-20250810163811-33a2a4c9', 'error_msg': 'Not Found. Please check the configuration.'}"
     ]
    }
   ],
   "source": [
    "#RAGå¢å¼ºç”Ÿæˆ\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/user_guide\")\n",
    "docs = loader.load()\n",
    "print(f\"Loaded {len(docs)} documents\")\n",
    "documents = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200\n",
    ").split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, OpenAIEmbeddings())\n",
    "retriever = vector.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='The ability to rapidly understand how the model is performing â€šÃ„Ã® and debug where it is failing â€šÃ„Ã® is incredibly important for this phase.Debuggingâ€šÃ„Ã£When developing new LLM applications, we suggest having LangSmith tracing enabled by default.\\nOftentimes, it isnâ€šÃ„Ã´t necessary to look at every single trace. However, when things go wrong (an unexpected end result, infinite agent loop, slower than expected execution, higher than expected token usage), itâ€šÃ„Ã´s extremely helpful to debug by looking through the application traces. LangSmith gives clear visibility and debugging information at each step of an LLM sequence, making it much easier to identify and root-cause issues.', metadata={'source': 'https://docs.smith.langchain.com/user_guide', 'title': 'LangSmith User Guide | \\uf8ffÃ¼Â¶ÃºÃ”âˆÃ¨\\uf8ffÃ¼Ãµâ€ Ã”âˆÃ¨ LangSmith', 'description': 'LangSmith is a platform for LLM application development, monitoring, and testing. In this guide, weâ€šÃ„Ã´ll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if theyâ€šÃ„Ã´re just starting their journey.', 'language': 'en'})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#æœç´¢åŒ¹é…æ–‡æ¡£å—\n",
    "retriever.get_relevant_documents(\"å¦‚ä½•debug?\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tool(name='langsmith_search', description='æœç´¢æœ‰å…³ LangSmith çš„ä¿¡æ¯ã€‚å…³äºLangSmithçš„ä»»ä½•é—®é¢˜ï¼Œä½ ä¸€å®šè¦ä½¿ç”¨è¿™ä¸ªå·¥å…·ï¼', args_schema=<class 'langchain.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x14e139da0>, retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x108502cd0>), document_prompt=PromptTemplate(input_variables=['page_content'], template='{page_content}'), document_separator='\\n\\n'), coroutine=functools.partial(<function _aget_relevant_documents at 0x14e13a0c0>, retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x108502cd0>), document_prompt=PromptTemplate(input_variables=['page_content'], template='{page_content}'), document_separator='\\n\\n'))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#æŠŠæ£€ç´¢å™¨åŠ å…¥åˆ°å·¥å…·ä¸­\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"langsmith_search\",\n",
    "    \"æœç´¢æœ‰å…³ LangSmith çš„ä¿¡æ¯ã€‚å…³äºLangSmithçš„ä»»ä½•é—®é¢˜ï¼Œä½ ä¸€å®šè¦ä½¿ç”¨è¿™ä¸ªå·¥å…·ï¼\",\n",
    ")\n",
    "retriever_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructuredTool(name='search', description='search(query: str) -> str - å½“éœ€è¦æŸ¥æ‰¾å®æ—¶ä¿¡æ¯çš„æ—¶å€™æ‰ä¼šä½¿ç”¨è¿™ä¸ªå·¥å…·.', args_schema=<class 'pydantic.v1.main.searchSchemaSchema'>, func=<function search at 0x14e9adbc0>),\n",
       " Tool(name='langsmith_search', description='æœç´¢æœ‰å…³ LangSmith çš„ä¿¡æ¯ã€‚å…³äºLangSmithçš„ä»»ä½•é—®é¢˜ï¼Œä½ ä¸€å®šè¦ä½¿ç”¨è¿™ä¸ªå·¥å…·ï¼', args_schema=<class 'langchain.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x14e139da0>, retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x108502cd0>), document_prompt=PromptTemplate(input_variables=['page_content'], template='{page_content}'), document_separator='\\n\\n'), coroutine=functools.partial(<function _aget_relevant_documents at 0x14e13a0c0>, retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x108502cd0>), document_prompt=PromptTemplate(input_variables=['page_content'], template='{page_content}'), document_separator='\\n\\n'))]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#å¯ç”¨å·¥å…·é›†\n",
    "tools = [search, retriever_tool]\n",
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#å®šä¹‰æ¨¡å‹\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')),\n",
       " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ä»hubä¸­è·å–æ¨¡ç‰ˆ\n",
    "from langchain import hub\n",
    "\n",
    "# ä¸€ä¸ªæœ€ç®€å•çš„æ¨¡ç‰ˆ,å¸¦è®°å¿†\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#åˆ›å»ºagent\n",
    "from langchain.agents import create_openai_functions_agent\n",
    "agent = create_openai_functions_agent(llm, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#åˆ›å»ºagentæ‰§è¡Œå™¨AgentExecutor\n",
    "from langchain.agents import AgentExecutor\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mHello! How can I assist you today?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'hi!', 'output': 'Hello! How can I assist you today?'}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#æ‰§è¡Œ\n",
    "agent_executor.invoke({\"input\": \"hi!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `langsmith_search` with `{'query': '\\nlangsmith project testing'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mWe provide native rendering of chat messages, functions, and retrieve documents.Initial Test Setâ€šÃ„Ã£While many developers still ship an initial version of their application based on â€šÃ„Ãºvibe checksâ€šÃ„Ã¹, weâ€šÃ„Ã´ve seen an increasing number of engineering teams start to adopt a more test driven approach. LangSmith allows developers to create datasets, which are collections of inputs and reference outputs, and use these to run tests on their LLM applications.\n",
      "These test cases can be uploaded in bulk, created on the fly, or exported from application traces. LangSmith also makes it easy to run custom evaluations (both LLM and heuristic based) to score test results.Comparison Viewâ€šÃ„Ã£When prototyping different versions of your applications and making changes, itâ€šÃ„Ã´s important to see whether or not youâ€šÃ„Ã´ve regressed with respect to your initial test cases.\n",
      "Oftentimes, changes in the prompt, retrieval strategy, or model choice can have huge implications in responses produced by your application.\n",
      "\n",
      "Skip to main contentï£¿Ã¼Â¶ÃºÃ”âˆÃ¨ï£¿Ã¼Ãµâ€ Ã”âˆÃ¨ LangSmith DocsLangChain Python DocsLangChain JS/TS DocsLangSmith API DocsSearchGo to AppLangSmithUser GuideSetupPricing (Coming Soon)Self-HostingTracingEvaluationMonitoringPrompt HubUser GuideOn this pageLangSmith User GuideLangSmith is a platform for LLM application development, monitoring, and testing. In this guide, weâ€šÃ„Ã´ll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if theyâ€šÃ„Ã´re just starting their journey.Prototypingâ€šÃ„Ã£Prototyping LLM applications often involves quick experimentation between prompts, model types, retrieval strategy and other parameters.\n",
      "\n",
      "cost, and feedback scores. This ensures that it's delivering desirable results at scale.Monitoring and A/B Testingâ€šÃ„Ã£LangSmith provides monitoring charts that allow you to track key metrics over time. You can expand to view metrics for a given period and drill down into a specific data point to get a trace table for that time period â€šÃ„Ã® this is especially handy for debugging production issues.LangSmith also allows for tag and metadata grouping, which allows users to mark different versions of their applications with different identifiers and view how they are performing side-by-side within each chart. This is helpful for A/B testing changes in prompt, model, or retrieval strategy.PreviousLangSmithNextSetupPrototypingBeta TestingProductionCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogCopyright Â¬Â© 2024 LangChain, Inc.\n",
      "\n",
      "LangSmith User Guide | ï£¿Ã¼Â¶ÃºÃ”âˆÃ¨ï£¿Ã¼Ãµâ€ Ã”âˆÃ¨ LangSmith\u001b[0m\u001b[32;1m\u001b[1;3mLangSmith can assist with project testing in several ways:\n",
      "\n",
      "1. **Creating Test Datasets**: Developers can create datasets that consist of collections of inputs and reference outputs. These datasets can be used to run tests on their LLM (Large Language Model) applications to ensure that the application behaves as expected.\n",
      "\n",
      "2. **Bulk Upload of Test Cases**: Test cases can be uploaded in bulk, which allows for efficient testing of multiple scenarios at once.\n",
      "\n",
      "3. **On-the-Fly Test Case Creation**: Developers can create test cases on the fly, which is useful for quickly testing new features or changes in the application.\n",
      "\n",
      "4. **Exporting from Application Traces**: Test cases can also be exported from application traces, which helps in creating tests based on real-world usage patterns.\n",
      "\n",
      "5. **Custom Evaluations**: LangSmith enables running custom evaluations, both LLM and heuristic-based, to score test results. This helps in assessing the performance of the application under various conditions.\n",
      "\n",
      "6. **Comparison View**: When making changes to the application, LangSmith provides a comparison view to see if there has been any regression with respect to the initial test cases. This is important when changes are made to prompts, retrieval strategies, or model choices.\n",
      "\n",
      "7. **Monitoring and A/B Testing**: LangSmith offers monitoring charts to track key metrics over time and allows for tag and metadata grouping. This feature is useful for marking different versions of applications and comparing their performance side-by-side, which is essential for A/B testing.\n",
      "\n",
      "Overall, LangSmith supports a test-driven approach to LLM application development, providing tools for creating, managing, and evaluating test cases, as well as monitoring and comparing application performance over time.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'langsmithå¦‚ä½•å¸®åŠ©åšé¡¹ç›®æµ‹è¯•?',\n",
       " 'output': 'LangSmith can assist with project testing in several ways:\\n\\n1. **Creating Test Datasets**: Developers can create datasets that consist of collections of inputs and reference outputs. These datasets can be used to run tests on their LLM (Large Language Model) applications to ensure that the application behaves as expected.\\n\\n2. **Bulk Upload of Test Cases**: Test cases can be uploaded in bulk, which allows for efficient testing of multiple scenarios at once.\\n\\n3. **On-the-Fly Test Case Creation**: Developers can create test cases on the fly, which is useful for quickly testing new features or changes in the application.\\n\\n4. **Exporting from Application Traces**: Test cases can also be exported from application traces, which helps in creating tests based on real-world usage patterns.\\n\\n5. **Custom Evaluations**: LangSmith enables running custom evaluations, both LLM and heuristic-based, to score test results. This helps in assessing the performance of the application under various conditions.\\n\\n6. **Comparison View**: When making changes to the application, LangSmith provides a comparison view to see if there has been any regression with respect to the initial test cases. This is important when changes are made to prompts, retrieval strategies, or model choices.\\n\\n7. **Monitoring and A/B Testing**: LangSmith offers monitoring charts to track key metrics over time and allows for tag and metadata grouping. This feature is useful for marking different versions of applications and comparing their performance side-by-side, which is essential for A/B testing.\\n\\nOverall, LangSmith supports a test-driven approach to LLM application development, providing tools for creating, managing, and evaluating test cases, as well as monitoring and comparing application performance over time.'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"langsmithå¦‚ä½•å¸®åŠ©åšé¡¹ç›®æµ‹è¯•?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `search` with `{'query': '\\nBeijing weather today'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m{'type': 'weather_result', 'temperature': '53', 'unit': 'Fahrenheit', 'precipitation': '0%', 'humidity': '53%', 'wind': '5 mph', 'location': 'Beijing, China', 'date': 'Saturday', 'weather': 'Partly cloudy'}\u001b[0m\u001b[32;1m\u001b[1;3mä»Šå¤©åŒ—äº¬çš„å¤©æ°”æ˜¯éƒ¨åˆ†å¤šäº‘ï¼Œæ°”æ¸©ä¸º53åæ°åº¦ï¼ˆçº¦11.7æ‘„æ°åº¦ï¼‰ï¼Œé™æ°´æ¦‚ç‡ä¸º0%ï¼Œæ¹¿åº¦ä¸º53%ï¼Œé£é€Ÿä¸ºæ¯å°æ—¶5è‹±é‡Œã€‚\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'åŒ—äº¬ä»Šå¤©çš„å¤©æ°”å¦‚ä½•?',\n",
       " 'output': 'ä»Šå¤©åŒ—äº¬çš„å¤©æ°”æ˜¯éƒ¨åˆ†å¤šäº‘ï¼Œæ°”æ¸©ä¸º53åæ°åº¦ï¼ˆçº¦11.7æ‘„æ°åº¦ï¼‰ï¼Œé™æ°´æ¦‚ç‡ä¸º0%ï¼Œæ¹¿åº¦ä¸º53%ï¼Œé£é€Ÿä¸ºæ¯å°æ—¶5è‹±é‡Œã€‚'}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#å®æ—¶ä¿¡æ¯æé—®\n",
    "agent_executor.invoke({\"input\": \"åŒ—äº¬ä»Šå¤©çš„å¤©æ°”å¦‚ä½•?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3måˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬è¿˜æ²¡æœ‰å¼€å§‹ä»»ä½•è®¨è®ºã€‚è¿™æ˜¯æˆ‘ä»¬çš„ç¬¬ä¸€æ¬¡äº’åŠ¨ã€‚å¦‚æœæ‚¨æœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦å¸®åŠ©ï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ï¼Œæˆ‘ä¼šå°½åŠ›ååŠ©æ‚¨ï¼\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'æˆªæ­¢ç›®å‰æˆ‘ä»¬éƒ½èŠäº†ä»€ä¹ˆ?',\n",
       " 'output': 'åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬è¿˜æ²¡æœ‰å¼€å§‹ä»»ä½•è®¨è®ºã€‚è¿™æ˜¯æˆ‘ä»¬çš„ç¬¬ä¸€æ¬¡äº’åŠ¨ã€‚å¦‚æœæ‚¨æœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦å¸®åŠ©ï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ï¼Œæˆ‘ä¼šå°½åŠ›ååŠ©æ‚¨ï¼'}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#å‰é¢çš„äº¤äº’éƒ½ä¸å¸¦è®°å¿†\n",
    "agent_executor.invoke({\"input\": \"æˆªæ­¢ç›®å‰æˆ‘ä»¬éƒ½èŠäº†ä»€ä¹ˆ?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mä½ å¥½ï¼ŒBobï¼å¾ˆé«˜å…´è®¤è¯†ä½ ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'hi! æˆ‘å«bob',\n",
       " 'chat_history': [],\n",
       " 'output': 'ä½ å¥½ï¼ŒBobï¼å¾ˆé«˜å…´è®¤è¯†ä½ ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ'}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#äº¤äº’æ—¶æ·»åŠ è®°å¿†\n",
    "agent_executor.invoke({\"input\": \"hi! æˆ‘å«bob\", \"chat_history\": []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mä½ çš„åå­—æ˜¯Bobã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿ\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='hi! æˆ‘å«bob'),\n",
       "  AIMessage(content='ä½ å¥½ï¼ŒBobï¼å¾ˆé«˜å…´è®¤è¯†ä½ ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ')],\n",
       " 'input': 'æˆ‘å«ä»€ä¹ˆåå­—?',\n",
       " 'output': 'ä½ çš„åå­—æ˜¯Bobã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿ'}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#æ‰‹åŠ¨æ„é€ è®°å¿†æ•°æ®\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(content=\"hi! æˆ‘å«bob\"),\n",
    "            AIMessage(content=\"ä½ å¥½ï¼ŒBobï¼å¾ˆé«˜å…´è®¤è¯†ä½ ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ\"),\n",
    "        ],\n",
    "        \"input\": \"æˆ‘å«ä»€ä¹ˆåå­—?\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ä½¿ç”¨RunnableWithMessageHistoryè‡ªåŠ¨æ„é€ è®°å¿†æ•°æ®\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "message_history = ChatMessageHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_with_chat_history = RunnableWithMessageHistory(\n",
    "    agent_executor,\n",
    "    #æ³¨æ„æ­¤å¤„session_idæ²¡æœ‰ç”¨åˆ°ï¼Œå› ä¸ºæˆ‘ä»¬æ²¡ç”¨ç±»ä¼¼redisçš„å­˜å‚¨,åªæ˜¯ç”¨äº†ä¸€ä¸ªå˜é‡\n",
    "    lambda session_id: message_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mä½ å¥½ï¼ŒTomieï¼å¾ˆé«˜å…´è§åˆ°ä½ ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'hi! æˆ‘æ˜¯tomie',\n",
       " 'chat_history': [],\n",
       " 'output': 'ä½ å¥½ï¼ŒTomieï¼å¾ˆé«˜å…´è§åˆ°ä½ ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ'}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#è°ƒç”¨æ–¹å¼\n",
    "agent_with_chat_history.invoke(\n",
    "    {\"input\": \"hi! æˆ‘æ˜¯tomie\"},\n",
    "    #æ³¨æ„æ­¤å¤„session_idæ²¡æœ‰ç”¨åˆ°ï¼Œå› ä¸ºæˆ‘ä»¬æ²¡ç”¨ç±»ä¼¼redisçš„å­˜å‚¨,åªæ˜¯ç”¨äº†ä¸€ä¸ªå˜é‡\n",
    "    config={\"configurable\": {\"session_id\": \"foo_001\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mä½ åˆšæ‰å‘Šè¯‰æˆ‘ä½ çš„åå­—æ˜¯Tomieã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿ\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'æˆ‘å«ä»€ä¹ˆåå­—?',\n",
       " 'chat_history': [HumanMessage(content='hi! æˆ‘æ˜¯tomie'),\n",
       "  AIMessage(content='ä½ å¥½ï¼ŒTomieï¼å¾ˆé«˜å…´è§åˆ°ä½ ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ')],\n",
       " 'output': 'ä½ åˆšæ‰å‘Šè¯‰æˆ‘ä½ çš„åå­—æ˜¯Tomieã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿ'}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#è°ƒç”¨æ–¹å¼\n",
    "agent_with_chat_history.invoke(\n",
    "    {\"input\": \"æˆ‘å«ä»€ä¹ˆåå­—?\"},\n",
    "    #æ³¨æ„æ­¤å¤„session_idæ²¡æœ‰ç”¨åˆ°ï¼Œå› ä¸ºæˆ‘ä»¬æ²¡ç”¨ç±»ä¼¼redisçš„å­˜å‚¨,åªæ˜¯ç”¨äº†ä¸€ä¸ªå˜é‡\n",
    "    config={\"configurable\": {\"session_id\": \"foo_001\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `langsmith_search` with `{'query': 'LangSmith \\n'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mLangSmith User Guide | ï£¿Ã¼Â¶ÃºÃ”âˆÃ¨ï£¿Ã¼Ãµâ€ Ã”âˆÃ¨ LangSmith\n",
      "\n",
      "Skip to main contentï£¿Ã¼Â¶ÃºÃ”âˆÃ¨ï£¿Ã¼Ãµâ€ Ã”âˆÃ¨ LangSmith DocsLangChain Python DocsLangChain JS/TS DocsLangSmith API DocsSearchGo to AppLangSmithUser GuideSetupPricing (Coming Soon)Self-HostingTracingEvaluationMonitoringPrompt HubUser GuideOn this pageLangSmith User GuideLangSmith is a platform for LLM application development, monitoring, and testing. In this guide, weâ€šÃ„Ã´ll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if theyâ€šÃ„Ã´re just starting their journey.Prototypingâ€šÃ„Ã£Prototyping LLM applications often involves quick experimentation between prompts, model types, retrieval strategy and other parameters.\n",
      "\n",
      "We provide native rendering of chat messages, functions, and retrieve documents.Initial Test Setâ€šÃ„Ã£While many developers still ship an initial version of their application based on â€šÃ„Ãºvibe checksâ€šÃ„Ã¹, weâ€šÃ„Ã´ve seen an increasing number of engineering teams start to adopt a more test driven approach. LangSmith allows developers to create datasets, which are collections of inputs and reference outputs, and use these to run tests on their LLM applications.\n",
      "These test cases can be uploaded in bulk, created on the fly, or exported from application traces. LangSmith also makes it easy to run custom evaluations (both LLM and heuristic based) to score test results.Comparison Viewâ€šÃ„Ã£When prototyping different versions of your applications and making changes, itâ€šÃ„Ã´s important to see whether or not youâ€šÃ„Ã´ve regressed with respect to your initial test cases.\n",
      "Oftentimes, changes in the prompt, retrieval strategy, or model choice can have huge implications in responses produced by your application.\n",
      "\n",
      "The ability to rapidly understand how the model is performing â€šÃ„Ã® and debug where it is failing â€šÃ„Ã® is incredibly important for this phase.Debuggingâ€šÃ„Ã£When developing new LLM applications, we suggest having LangSmith tracing enabled by default.\n",
      "Oftentimes, it isnâ€šÃ„Ã´t necessary to look at every single trace. However, when things go wrong (an unexpected end result, infinite agent loop, slower than expected execution, higher than expected token usage), itâ€šÃ„Ã´s extremely helpful to debug by looking through the application traces. LangSmith gives clear visibility and debugging information at each step of an LLM sequence, making it much easier to identify and root-cause issues.\u001b[0m\u001b[32;1m\u001b[1;3mLangSmithæ˜¯ä¸€ä¸ªç”¨äºLLMï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰åº”ç”¨å¼€å‘ã€ç›‘æ§å’Œæµ‹è¯•çš„å¹³å°ã€‚ä»¥ä¸‹æ˜¯å¦‚ä½•ä½¿ç”¨LangSmithçš„ä¸€äº›åŸºæœ¬æŒ‡å—ï¼š\n",
      "\n",
      "1. **åŸå‹è®¾è®¡**ï¼šåœ¨LLMåº”ç”¨çš„åŸå‹è®¾è®¡é˜¶æ®µï¼Œé€šå¸¸æ¶‰åŠåˆ°å¿«é€Ÿåœ¨ä¸åŒçš„æç¤ºï¼ˆpromptsï¼‰ã€æ¨¡å‹ç±»å‹ã€æ£€ç´¢ç­–ç•¥å’Œå…¶ä»–å‚æ•°ä¹‹é—´è¿›è¡Œå®éªŒã€‚LangSmithæä¾›äº†èŠå¤©æ¶ˆæ¯ã€å‡½æ•°å’Œæ£€ç´¢æ–‡æ¡£çš„åŸç”Ÿæ¸²æŸ“ã€‚\n",
      "\n",
      "2. **åˆå§‹æµ‹è¯•é›†**ï¼šå¼€å‘è€…å¯ä»¥åˆ›å»ºæ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†æ˜¯è¾“å…¥å’Œå‚è€ƒè¾“å‡ºçš„é›†åˆï¼Œå¹¶ä½¿ç”¨è¿™äº›æ•°æ®é›†åœ¨LLMåº”ç”¨ä¸Šè¿è¡Œæµ‹è¯•ã€‚æµ‹è¯•ç”¨ä¾‹å¯ä»¥æ‰¹é‡ä¸Šä¼ ã€å³æ—¶åˆ›å»ºæˆ–ä»åº”ç”¨è·Ÿè¸ªä¸­å¯¼å‡ºã€‚LangSmithè¿˜å¯ä»¥è½»æ¾è¿è¡Œè‡ªå®šä¹‰è¯„ä¼°ï¼ˆåŸºäºLLMå’Œå¯å‘å¼çš„ï¼‰æ¥è¯„åˆ†æµ‹è¯•ç»“æœã€‚\n",
      "\n",
      "3. **æ¯”è¾ƒè§†å›¾**ï¼šåœ¨åŸå‹è®¾è®¡ä¸åŒç‰ˆæœ¬çš„åº”ç”¨å¹¶è¿›è¡Œæ›´æ”¹æ—¶ï¼Œé‡è¦çš„æ˜¯è¦çœ‹åˆ°ä½ æ˜¯å¦åœ¨åˆå§‹æµ‹è¯•ç”¨ä¾‹æ–¹é¢æœ‰æ‰€é€€æ­¥ã€‚æç¤ºã€æ£€ç´¢ç­–ç•¥æˆ–æ¨¡å‹é€‰æ‹©çš„å˜åŒ–å¯èƒ½å¯¹åº”ç”¨ç¨‹åºäº§ç”Ÿçš„å“åº”æœ‰å¾ˆå¤§å½±å“ã€‚å¿«é€Ÿç†è§£æ¨¡å‹çš„æ€§èƒ½å¹¶è°ƒè¯•å¤±è´¥çš„åœ°æ–¹å¯¹äºè¿™ä¸ªé˜¶æ®µéå¸¸é‡è¦ã€‚\n",
      "\n",
      "4. **è°ƒè¯•**ï¼šåœ¨å¼€å‘æ–°çš„LLMåº”ç”¨æ—¶ï¼Œå»ºè®®é»˜è®¤å¯ç”¨LangSmithè·Ÿè¸ªã€‚é€šå¸¸ä¸éœ€è¦æŸ¥çœ‹æ¯ä¸€ä¸ªè·Ÿè¸ªï¼Œä½†å½“å‡ºç°é—®é¢˜ï¼ˆæ„å¤–çš„ç»“æœã€æ— é™ä»£ç†å¾ªç¯ã€æ‰§è¡Œé€Ÿåº¦æ…¢äºé¢„æœŸã€ä»¤ç‰Œä½¿ç”¨é‡é«˜äºé¢„æœŸï¼‰æ—¶ï¼Œé€šè¿‡æŸ¥çœ‹åº”ç”¨è·Ÿè¸ªæ¥è°ƒè¯•éå¸¸æœ‰å¸®åŠ©ã€‚LangSmithåœ¨LLMåºåˆ—çš„æ¯ä¸€æ­¥æä¾›æ¸…æ™°çš„å¯è§æ€§å’Œè°ƒè¯•ä¿¡æ¯ï¼Œä½¿å¾—è¯†åˆ«å’Œæ‰¾å‡ºé—®é¢˜æ ¹æºå˜å¾—æ›´åŠ å®¹æ˜“ã€‚\n",
      "\n",
      "è¿™äº›æ˜¯LangSmithçš„ä¸€äº›åŸºæœ¬ä½¿ç”¨æŒ‡å—ã€‚å¦‚æœä½ æœ‰æ›´å…·ä½“çš„é—®é¢˜æˆ–éœ€è¦è¿›ä¸€æ­¥çš„å¸®åŠ©ï¼Œè¯·å‘Šè¯‰æˆ‘ï¼\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'LangSmithå¦‚ä½•ä½¿ç”¨?',\n",
       " 'chat_history': [HumanMessage(content='hi! æˆ‘æ˜¯tomie'),\n",
       "  AIMessage(content='ä½ å¥½ï¼ŒTomieï¼å¾ˆé«˜å…´è§åˆ°ä½ ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ'),\n",
       "  HumanMessage(content='æˆ‘å«ä»€ä¹ˆåå­—?'),\n",
       "  AIMessage(content='ä½ åˆšæ‰å‘Šè¯‰æˆ‘ä½ çš„åå­—æ˜¯Tomieã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿ')],\n",
       " 'output': 'LangSmithæ˜¯ä¸€ä¸ªç”¨äºLLMï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰åº”ç”¨å¼€å‘ã€ç›‘æ§å’Œæµ‹è¯•çš„å¹³å°ã€‚ä»¥ä¸‹æ˜¯å¦‚ä½•ä½¿ç”¨LangSmithçš„ä¸€äº›åŸºæœ¬æŒ‡å—ï¼š\\n\\n1. **åŸå‹è®¾è®¡**ï¼šåœ¨LLMåº”ç”¨çš„åŸå‹è®¾è®¡é˜¶æ®µï¼Œé€šå¸¸æ¶‰åŠåˆ°å¿«é€Ÿåœ¨ä¸åŒçš„æç¤ºï¼ˆpromptsï¼‰ã€æ¨¡å‹ç±»å‹ã€æ£€ç´¢ç­–ç•¥å’Œå…¶ä»–å‚æ•°ä¹‹é—´è¿›è¡Œå®éªŒã€‚LangSmithæä¾›äº†èŠå¤©æ¶ˆæ¯ã€å‡½æ•°å’Œæ£€ç´¢æ–‡æ¡£çš„åŸç”Ÿæ¸²æŸ“ã€‚\\n\\n2. **åˆå§‹æµ‹è¯•é›†**ï¼šå¼€å‘è€…å¯ä»¥åˆ›å»ºæ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†æ˜¯è¾“å…¥å’Œå‚è€ƒè¾“å‡ºçš„é›†åˆï¼Œå¹¶ä½¿ç”¨è¿™äº›æ•°æ®é›†åœ¨LLMåº”ç”¨ä¸Šè¿è¡Œæµ‹è¯•ã€‚æµ‹è¯•ç”¨ä¾‹å¯ä»¥æ‰¹é‡ä¸Šä¼ ã€å³æ—¶åˆ›å»ºæˆ–ä»åº”ç”¨è·Ÿè¸ªä¸­å¯¼å‡ºã€‚LangSmithè¿˜å¯ä»¥è½»æ¾è¿è¡Œè‡ªå®šä¹‰è¯„ä¼°ï¼ˆåŸºäºLLMå’Œå¯å‘å¼çš„ï¼‰æ¥è¯„åˆ†æµ‹è¯•ç»“æœã€‚\\n\\n3. **æ¯”è¾ƒè§†å›¾**ï¼šåœ¨åŸå‹è®¾è®¡ä¸åŒç‰ˆæœ¬çš„åº”ç”¨å¹¶è¿›è¡Œæ›´æ”¹æ—¶ï¼Œé‡è¦çš„æ˜¯è¦çœ‹åˆ°ä½ æ˜¯å¦åœ¨åˆå§‹æµ‹è¯•ç”¨ä¾‹æ–¹é¢æœ‰æ‰€é€€æ­¥ã€‚æç¤ºã€æ£€ç´¢ç­–ç•¥æˆ–æ¨¡å‹é€‰æ‹©çš„å˜åŒ–å¯èƒ½å¯¹åº”ç”¨ç¨‹åºäº§ç”Ÿçš„å“åº”æœ‰å¾ˆå¤§å½±å“ã€‚å¿«é€Ÿç†è§£æ¨¡å‹çš„æ€§èƒ½å¹¶è°ƒè¯•å¤±è´¥çš„åœ°æ–¹å¯¹äºè¿™ä¸ªé˜¶æ®µéå¸¸é‡è¦ã€‚\\n\\n4. **è°ƒè¯•**ï¼šåœ¨å¼€å‘æ–°çš„LLMåº”ç”¨æ—¶ï¼Œå»ºè®®é»˜è®¤å¯ç”¨LangSmithè·Ÿè¸ªã€‚é€šå¸¸ä¸éœ€è¦æŸ¥çœ‹æ¯ä¸€ä¸ªè·Ÿè¸ªï¼Œä½†å½“å‡ºç°é—®é¢˜ï¼ˆæ„å¤–çš„ç»“æœã€æ— é™ä»£ç†å¾ªç¯ã€æ‰§è¡Œé€Ÿåº¦æ…¢äºé¢„æœŸã€ä»¤ç‰Œä½¿ç”¨é‡é«˜äºé¢„æœŸï¼‰æ—¶ï¼Œé€šè¿‡æŸ¥çœ‹åº”ç”¨è·Ÿè¸ªæ¥è°ƒè¯•éå¸¸æœ‰å¸®åŠ©ã€‚LangSmithåœ¨LLMåºåˆ—çš„æ¯ä¸€æ­¥æä¾›æ¸…æ™°çš„å¯è§æ€§å’Œè°ƒè¯•ä¿¡æ¯ï¼Œä½¿å¾—è¯†åˆ«å’Œæ‰¾å‡ºé—®é¢˜æ ¹æºå˜å¾—æ›´åŠ å®¹æ˜“ã€‚\\n\\nè¿™äº›æ˜¯LangSmithçš„ä¸€äº›åŸºæœ¬ä½¿ç”¨æŒ‡å—ã€‚å¦‚æœä½ æœ‰æ›´å…·ä½“çš„é—®é¢˜æˆ–éœ€è¦è¿›ä¸€æ­¥çš„å¸®åŠ©ï¼Œè¯·å‘Šè¯‰æˆ‘ï¼'}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#è°ƒç”¨æ–¹å¼\n",
    "agent_with_chat_history.invoke(\n",
    "    {\"input\": \"LangSmithå¦‚ä½•ä½¿ç”¨?\"},\n",
    "    #æ³¨æ„æ­¤å¤„session_idæ²¡æœ‰ç”¨åˆ°ï¼Œå› ä¸ºæˆ‘ä»¬æ²¡ç”¨ç±»ä¼¼redisçš„å­˜å‚¨,åªæ˜¯ç”¨äº†ä¸€ä¸ªå˜é‡\n",
    "    config={\"configurable\": {\"session_id\": \"foo_001\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3måˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬çš„å¯¹è¯å†…å®¹å¦‚ä¸‹ï¼š\n",
      "\n",
      "1. ä½ å‘æˆ‘æ‰“æ‹›å‘¼ï¼Œå¹¶å‘Šè¯‰æˆ‘ä½ çš„åå­—æ˜¯Tomieã€‚\n",
      "2. æˆ‘é—®ä½ æ˜¯å¦éœ€è¦å¸®åŠ©ã€‚\n",
      "3. ä½ è¯¢é—®äº†ä½ çš„åå­—ï¼Œæˆ‘ç¡®è®¤äº†ä½ ä¹‹å‰å‘Šè¯‰æˆ‘çš„åå­—æ˜¯Tomieã€‚\n",
      "4. ä½ é—®äº†å…³äºLangSmithå¦‚ä½•ä½¿ç”¨çš„é—®é¢˜ï¼Œæˆ‘ç»™ä½ æä¾›äº†å…³äºLangSmithä½¿ç”¨çš„ä¸€äº›åŸºæœ¬æŒ‡å—ã€‚\n",
      "\n",
      "å¦‚æœä½ æœ‰å…¶ä»–é—®é¢˜æˆ–éœ€è¦è¿›ä¸€æ­¥çš„å¸®åŠ©ï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ï¼\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'æˆªæ­¢ç›®å‰æˆ‘ä»¬éƒ½èŠäº†ä»€ä¹ˆ?',\n",
       " 'chat_history': [HumanMessage(content='hi! æˆ‘æ˜¯tomie'),\n",
       "  AIMessage(content='ä½ å¥½ï¼ŒTomieï¼å¾ˆé«˜å…´è§åˆ°ä½ ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ'),\n",
       "  HumanMessage(content='æˆ‘å«ä»€ä¹ˆåå­—?'),\n",
       "  AIMessage(content='ä½ åˆšæ‰å‘Šè¯‰æˆ‘ä½ çš„åå­—æ˜¯Tomieã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿ'),\n",
       "  HumanMessage(content='LangSmithå¦‚ä½•ä½¿ç”¨?'),\n",
       "  AIMessage(content='LangSmithæ˜¯ä¸€ä¸ªç”¨äºLLMï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰åº”ç”¨å¼€å‘ã€ç›‘æ§å’Œæµ‹è¯•çš„å¹³å°ã€‚ä»¥ä¸‹æ˜¯å¦‚ä½•ä½¿ç”¨LangSmithçš„ä¸€äº›åŸºæœ¬æŒ‡å—ï¼š\\n\\n1. **åŸå‹è®¾è®¡**ï¼šåœ¨LLMåº”ç”¨çš„åŸå‹è®¾è®¡é˜¶æ®µï¼Œé€šå¸¸æ¶‰åŠåˆ°å¿«é€Ÿåœ¨ä¸åŒçš„æç¤ºï¼ˆpromptsï¼‰ã€æ¨¡å‹ç±»å‹ã€æ£€ç´¢ç­–ç•¥å’Œå…¶ä»–å‚æ•°ä¹‹é—´è¿›è¡Œå®éªŒã€‚LangSmithæä¾›äº†èŠå¤©æ¶ˆæ¯ã€å‡½æ•°å’Œæ£€ç´¢æ–‡æ¡£çš„åŸç”Ÿæ¸²æŸ“ã€‚\\n\\n2. **åˆå§‹æµ‹è¯•é›†**ï¼šå¼€å‘è€…å¯ä»¥åˆ›å»ºæ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†æ˜¯è¾“å…¥å’Œå‚è€ƒè¾“å‡ºçš„é›†åˆï¼Œå¹¶ä½¿ç”¨è¿™äº›æ•°æ®é›†åœ¨LLMåº”ç”¨ä¸Šè¿è¡Œæµ‹è¯•ã€‚æµ‹è¯•ç”¨ä¾‹å¯ä»¥æ‰¹é‡ä¸Šä¼ ã€å³æ—¶åˆ›å»ºæˆ–ä»åº”ç”¨è·Ÿè¸ªä¸­å¯¼å‡ºã€‚LangSmithè¿˜å¯ä»¥è½»æ¾è¿è¡Œè‡ªå®šä¹‰è¯„ä¼°ï¼ˆåŸºäºLLMå’Œå¯å‘å¼çš„ï¼‰æ¥è¯„åˆ†æµ‹è¯•ç»“æœã€‚\\n\\n3. **æ¯”è¾ƒè§†å›¾**ï¼šåœ¨åŸå‹è®¾è®¡ä¸åŒç‰ˆæœ¬çš„åº”ç”¨å¹¶è¿›è¡Œæ›´æ”¹æ—¶ï¼Œé‡è¦çš„æ˜¯è¦çœ‹åˆ°ä½ æ˜¯å¦åœ¨åˆå§‹æµ‹è¯•ç”¨ä¾‹æ–¹é¢æœ‰æ‰€é€€æ­¥ã€‚æç¤ºã€æ£€ç´¢ç­–ç•¥æˆ–æ¨¡å‹é€‰æ‹©çš„å˜åŒ–å¯èƒ½å¯¹åº”ç”¨ç¨‹åºäº§ç”Ÿçš„å“åº”æœ‰å¾ˆå¤§å½±å“ã€‚å¿«é€Ÿç†è§£æ¨¡å‹çš„æ€§èƒ½å¹¶è°ƒè¯•å¤±è´¥çš„åœ°æ–¹å¯¹äºè¿™ä¸ªé˜¶æ®µéå¸¸é‡è¦ã€‚\\n\\n4. **è°ƒè¯•**ï¼šåœ¨å¼€å‘æ–°çš„LLMåº”ç”¨æ—¶ï¼Œå»ºè®®é»˜è®¤å¯ç”¨LangSmithè·Ÿè¸ªã€‚é€šå¸¸ä¸éœ€è¦æŸ¥çœ‹æ¯ä¸€ä¸ªè·Ÿè¸ªï¼Œä½†å½“å‡ºç°é—®é¢˜ï¼ˆæ„å¤–çš„ç»“æœã€æ— é™ä»£ç†å¾ªç¯ã€æ‰§è¡Œé€Ÿåº¦æ…¢äºé¢„æœŸã€ä»¤ç‰Œä½¿ç”¨é‡é«˜äºé¢„æœŸï¼‰æ—¶ï¼Œé€šè¿‡æŸ¥çœ‹åº”ç”¨è·Ÿè¸ªæ¥è°ƒè¯•éå¸¸æœ‰å¸®åŠ©ã€‚LangSmithåœ¨LLMåºåˆ—çš„æ¯ä¸€æ­¥æä¾›æ¸…æ™°çš„å¯è§æ€§å’Œè°ƒè¯•ä¿¡æ¯ï¼Œä½¿å¾—è¯†åˆ«å’Œæ‰¾å‡ºé—®é¢˜æ ¹æºå˜å¾—æ›´åŠ å®¹æ˜“ã€‚\\n\\nè¿™äº›æ˜¯LangSmithçš„ä¸€äº›åŸºæœ¬ä½¿ç”¨æŒ‡å—ã€‚å¦‚æœä½ æœ‰æ›´å…·ä½“çš„é—®é¢˜æˆ–éœ€è¦è¿›ä¸€æ­¥çš„å¸®åŠ©ï¼Œè¯·å‘Šè¯‰æˆ‘ï¼')],\n",
       " 'output': 'åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬çš„å¯¹è¯å†…å®¹å¦‚ä¸‹ï¼š\\n\\n1. ä½ å‘æˆ‘æ‰“æ‹›å‘¼ï¼Œå¹¶å‘Šè¯‰æˆ‘ä½ çš„åå­—æ˜¯Tomieã€‚\\n2. æˆ‘é—®ä½ æ˜¯å¦éœ€è¦å¸®åŠ©ã€‚\\n3. ä½ è¯¢é—®äº†ä½ çš„åå­—ï¼Œæˆ‘ç¡®è®¤äº†ä½ ä¹‹å‰å‘Šè¯‰æˆ‘çš„åå­—æ˜¯Tomieã€‚\\n4. ä½ é—®äº†å…³äºLangSmithå¦‚ä½•ä½¿ç”¨çš„é—®é¢˜ï¼Œæˆ‘ç»™ä½ æä¾›äº†å…³äºLangSmithä½¿ç”¨çš„ä¸€äº›åŸºæœ¬æŒ‡å—ã€‚\\n\\nå¦‚æœä½ æœ‰å…¶ä»–é—®é¢˜æˆ–éœ€è¦è¿›ä¸€æ­¥çš„å¸®åŠ©ï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ï¼'}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#è°ƒç”¨æ–¹å¼\n",
    "agent_with_chat_history.invoke(\n",
    "    {\"input\": \"æˆªæ­¢ç›®å‰æˆ‘ä»¬éƒ½èŠäº†ä»€ä¹ˆ?\"},\n",
    "    #æ³¨æ„æ­¤å¤„session_idæ²¡æœ‰ç”¨åˆ°ï¼Œå› ä¸ºæˆ‘ä»¬æ²¡ç”¨ç±»ä¼¼redisçš„å­˜å‚¨,åªæ˜¯ç”¨äº†ä¸€ä¸ªå˜é‡\n",
    "    config={\"configurable\": {\"session_id\": \"foo_001\"}},\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
